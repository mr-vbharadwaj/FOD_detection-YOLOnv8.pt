
# ðŸ›« FOD Detection using YOLOv8 (Local, CPU-Based)

This repository contains a **complete, end-to-end Foreign Object Debris (FOD) detection pipeline** built using **YOLOv8**.  
The project is designed to run **entirely on a local Windows machine**, without relying on cloud services or GPUs.

It demonstrates the full workflow from:
- dataset understanding
- model training
- inference on new images and videos
- object counting and reporting

This project is suitable for **learning, prototyping, academic use, and early-stage product exploration**.

---

## ðŸ“ Repository Structure

```
FOD/
â”œâ”€ .ipynb_checkpoints/              # Auto-generated by Jupyter (ignore)
â”‚  â”œâ”€ 01_fod_dataset_understanding-checkpoint.ipynb
â”‚  â”œâ”€ 02_train_fod_yolov8-checkpoint.ipynb
â”‚  â””â”€ 03_fod_inference_and_counting-checkpoint.ipynb
â”‚
â”œâ”€ 01_fod_dataset_understanding.ipynb   # Dataset visualization & sanity checks
â”œâ”€ 02_train_fod_yolov8.ipynb             # Model training notebook
â”œâ”€ 03_fod_inference_and_counting.ipynb   # Image/video inference + counting
â”‚
â”œâ”€ runs/                                 # YOLO training & inference outputs
â”œâ”€ test_files/                           # New images/videos for testing
â”œâ”€ yolov8n.pt                            # Pretrained YOLOv8 nano base model
â””â”€ README.md                             # Project documentation
```

---

## ðŸ§  Project Overview

The system performs **generic FOD detection** on runway imagery.

- **Model**: YOLOv8 Nano (Ultralytics)
- **Classes**: Single class (`FOD`)
- **Inputs**: Images, videos, live camera feed
- **Outputs**:
  - Bounding boxes
  - Confidence scores
  - Count of detected FOD objects

âš ï¸ This project performs **binary detection only**:
> FOD vs background

It does **not** classify debris types (bolt, plastic, metal, etc.).

---

## ðŸ’» System Requirements

### Minimum Tested Configuration

- **OS**: Windows 10 / 11
- **Python**: 3.9 â€“ 3.11
- **RAM**: 16 GB recommended
- **GPU**: Optional (CPU-only supported)
- **Disk Space**: ~5â€“10 GB

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Create a Virtual Environment

Open Command Prompt or PowerShell in the project directory:

```bat
python -m venv venv
venv\Scripts\activate
```

You should see:
```
(venv)
```

---

### 2ï¸âƒ£ Install Dependencies

```bat
pip install ultralytics opencv-python matplotlib pyyaml jupyter
```

This installs:
- YOLOv8 (Ultralytics)
- OpenCV (image/video processing)
- Jupyter Notebook
- Supporting utilities

---

### 3ï¸âƒ£ Launch Jupyter Notebook

```bat
jupyter notebook
```

A browser window will open automatically.

---

## ðŸ“˜ Notebook Execution Order (IMPORTANT)

Run the notebooks **in the following order**.

### 1ï¸âƒ£ Dataset Understanding

```
01_fod_dataset_understanding.ipynb
```

Purpose:
- Visualize training images
- Understand YOLO annotation format
- Verify bounding boxes and labels
- Inspect object size and dataset quality

ðŸ‘‰ **Do not skip this step.**

---

### 2ï¸âƒ£ Model Training

```
02_train_fod_yolov8.ipynb
```

Purpose:
- Load pretrained YOLOv8 nano model
- Fine-tune it for FOD detection
- Save trained weights

Training outputs are saved under:
```
runs/
 â””â”€ yolov8n_fod_fast_test/
    â””â”€ weights/
       â””â”€ best.pt
```

Notes:
- Optimized for CPU-only systems
- Reduced image size and dataset fraction for faster training

---

### 3ï¸âƒ£ Inference and Counting

```
03_fod_inference_and_counting.ipynb
```

Purpose:
- Load trained model (`best.pt`)
- Run inference on:
  - unseen images
  - videos
  - webcam input
- Count detected FOD objects
- Overlay results visually

This notebook represents the **end goal of the project**.

---

## ðŸ–¼ï¸ Testing on New Images

1. Place new images inside:
```
test_files/
```

Example:
```
test_files/runway_sample.jpg
```

2. Update the image path in `03_fod_inference_and_counting.ipynb`.
3. Run inference cells.

âœ… Test images must NOT be part of training data.

---

## ðŸŽ¥ Testing on Videos

1. Place video files inside:
```
test_files/
```

2. Update:
```python
source="test_files/your_video.mp4"
```

3. Run inference.

Output videos are saved to:
```
runs/detect/predict/
```

---

## ðŸ“Š Understanding the Output

Each detection includes:
- Bounding box
- Label: `FOD`
- Confidence score (0â€“1)
- Total FOD count per image/frame

You can adjust detection sensitivity:
```python
conf=0.4
```

- Lower â†’ higher recall (more false positives)
- Higher â†’ higher precision (more misses)

---

## âš ï¸ Known Limitations

- Single-class detection only
- No temporal smoothing for video
- CPU inference is slower than GPU
- Not aviation-certified

These limitations are **expected and documented**.

---

## ðŸš€ Future Improvements

- Temporal smoothing across video frames
- Confidence-based alert logic
- Small-object detection optimization
- Conversion to a UI / application
- Multi-class debris detection (requires re-annotation)

---

## ðŸ§‘â€ðŸ’» Intended Audience

- Beginners learning computer vision
- Students and academic projects
- Engineers prototyping locally
- Early-stage R&D experimentation

---

## ðŸ“Œ Final Note

> This project focuses on **understanding and building the full pipeline**,  
not just training a model.

If you can run all three notebooks end-to-end,  
you have built a **complete object detection system**.
